{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d175cbac-7130-4ae7-aa2e-c2345be09b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cee0404d-5d27-4261-82d7-cad5eecf917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([i//50 for i in range(100)])\n",
    "\n",
    "buffer_size = 100\n",
    "task_id = 1 # python's off by one thing\n",
    "cpt = 2\n",
    "\n",
    "task_partitions = tensor.chunk(task_id)\n",
    "class_partitions = [partition for task_partition in task_partitions for partition in task_partition.chunk(cpt)] # a list of tensors partioned along new class lines\n",
    "\n",
    "buffer=[]\n",
    "# First we delete\n",
    "for i, class_partition in enumerate(class_partitions):\n",
    "    # number of samples per class at current task partition\n",
    "    samples_per_class = buffer_size // (task_id+1) // cpt\n",
    "    \n",
    "    # number of samples to be deleted from each class\n",
    "    x = np.abs(samples_per_class - class_partition.shape[0])\n",
    "    class_partition = class_partition[:-x]\n",
    "    class_partitions[i] = class_partition\n",
    "    \n",
    "# Second we add in new samples\n",
    "for i in range(2,4):\n",
    "    # Add in new class samples to buffer (add in ipm samples, ipm samples for new task calculated external of this overwrite function)\n",
    "    new_samples = i*torch.ones(samples_per_class)\n",
    "    class_partitions.append(new_samples)\n",
    "tensor = torch.cat(class_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5e1041-568f-49a3-a407-67c995283782",
   "metadata": {},
   "source": [
    "### Initial Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "760399e2-37d1-4335-a91d-a889f41ca8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make maximal use of Buffer at all time\n",
    "# Not perfect as sometimes buffer will empty itself if numbers dont work out\n",
    "\n",
    "def overwrite(buffer, buffer_size, task_id, cpt):\n",
    "    \n",
    "    task_partitions = buffer.chunk(task_id)\n",
    "    class_partitions = [partition for task_partition in task_partitions for partition in task_partition.chunk(cpt)] # a list of tensors partioned along new class lines\n",
    "    \n",
    "    # First we delete\n",
    "    for i, class_partition in enumerate(class_partitions):\n",
    "        # number of samples per class at current task partition\n",
    "        #samples_per_class = int(np.ceil(buffer_size / (task_id+1) / cpt))\n",
    "        if i % 2 == 0:\n",
    "            samples_per_class = int(np.floor(buffer_size / (task_id+1) / cpt))\n",
    "        else:\n",
    "            samples_per_class = int(np.ceil(buffer_size / (task_id+1) / cpt))\n",
    "        #print(samples_per_class)\n",
    "\n",
    "        # number of samples to be deleted from each class\n",
    "        x = np.abs(samples_per_class - class_partition.shape[0])\n",
    "        #print(x)\n",
    "        class_partition = class_partition[:-x]\n",
    "        class_partitions[i] = class_partition\n",
    "        \n",
    "    # Second we add in new samples\n",
    "    for i in range(task_id*cpt, task_id*cpt + cpt):\n",
    "        # Add in new class samples to buffer (add in ipm samples, ipm samples for new task calculated external of this overwrite function)\n",
    "        new_samples = i*torch.ones(samples_per_class)\n",
    "        class_partitions.append(new_samples)\n",
    "        \n",
    "    buffer = torch.cat(class_partitions)\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58bff401-12d6-4c4d-87b7-795f1b509430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples in buffer: 100\n",
      "(tensor([0, 1]), tensor([50, 50]))\n",
      "Examples in buffer: 100\n",
      "(tensor([0., 1., 2., 3.]), tensor([25, 25, 25, 25]))\n",
      "Examples in buffer: 100\n",
      "(tensor([0., 1., 2., 3., 4., 5.]), tensor([16, 17, 16, 17, 17, 17]))\n",
      "Examples in buffer: 101\n",
      "(tensor([0., 1., 2., 3., 4., 5., 6., 7.]), tensor([12, 13, 12, 13, 12, 13, 13, 13]))\n",
      "Examples in buffer: 100\n",
      "(tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]), tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10]))\n"
     ]
    }
   ],
   "source": [
    "buffer_size = 100\n",
    "n_tasks = 5\n",
    "cpt = 2\n",
    "\n",
    "for t in range(n_tasks):\n",
    "    if t == 0:\n",
    "        buffer = torch.tensor([i//50 for i in range(buffer_size)])\n",
    "        print('Examples in buffer: {}'.format(torch.unique(buffer, return_counts = True)[1].sum()))\n",
    "        print(torch.unique(buffer, return_counts = True))\n",
    "    else:\n",
    "        buffer = overwrite(buffer, buffer_size, task_id=t, cpt=cpt)\n",
    "        print('Examples in buffer: {}'.format(torch.unique(buffer, return_counts = True)[1].sum()))\n",
    "        print(torch.unique(buffer, return_counts = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca10f82-c0d3-4d1b-9b74-8ab71c819f17",
   "metadata": {},
   "source": [
    "### Second Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "095ed8f6-b560-4fc6-ae5a-f73c26e358e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 100\n",
    "labels = torch.cat([torch.zeros(16), torch.ones(17), 2*torch.ones(16), 3*torch.ones(17), 4*torch.ones(17), 5*torch.ones(17)], dim=0)\n",
    "data = torch.randn(100)\n",
    "task_id = 3 # task_id + 1 for python's off by one thing\n",
    "cpt = 2\n",
    "\n",
    "class_ids = torch.unique(labels)\n",
    "data_partitions = [data[torch.argwhere(labels == class_id)].squeeze() for class_id in class_ids]\n",
    "label_partitions = [labels[torch.argwhere(labels == class_id)].squeeze() for class_id in class_ids]\n",
    "\n",
    "# Delete Data\n",
    "for i in range(len(class_ids)):\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        samples_per_class = int(np.floor(buffer_size / (task_id+1) / cpt))\n",
    "    else:\n",
    "        samples_per_class = int(np.ceil(buffer_size / (task_id+1) / cpt))\n",
    "        \n",
    "    x = np.abs(samples_per_class - label_partitions[i].shape[0])\n",
    "    \n",
    "    data_partitions[i] = data_partitions[i][:-x]\n",
    "    label_partitions[i] = label_partitions[i][:-x]\n",
    "    \n",
    "data_buffer = torch.cat(data_partitions)\n",
    "label_buffer = torch.cat(label_partitions)\n",
    "\n",
    "# Add Data\n",
    "num_to_be_added = buffer_size - data_buffer.shape[0]\n",
    "\n",
    "for i in range(cpt*task_id, cpt*task_id + 2):\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        samples_to_add = int(np.floor(num_to_be_added / cpt))\n",
    "    else:\n",
    "        samples_to_add = int(np.ceil(num_to_be_added / cpt))\n",
    "        \n",
    "    data_buffer = torch.cat((data_buffer, torch.randn(samples_to_add)))\n",
    "    label_buffer = torch.cat((label_buffer, i*torch.ones(samples_to_add)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08999b2e-1c79-4cf4-8d11-85901d8625d2",
   "metadata": {},
   "source": [
    "#### Below only works for 5 tasks with 2 classes per task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bff80496-1183-40b5-a928-247b2bae2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, labels, and buffer_size all exist relative to buffer object (i.e., self.examples, self.labels, self.buffer_size)\n",
    "# we could also make cpt relative to buffer as well since this typically doesn't change in literature\n",
    "\n",
    "def delete_data(data_buffer, labels_buffer, buffer_size, task_id, cpt):\n",
    "    \n",
    "    class_ids = torch.unique(labels_buffer)\n",
    "    data_partitions = [data_buffer[torch.argwhere(labels_buffer == class_id)].squeeze() for class_id in class_ids]\n",
    "    label_partitions = [labels_buffer[torch.argwhere(labels_buffer == class_id)].squeeze() for class_id in class_ids]\n",
    "    \n",
    "    #if task_id == 7:\n",
    "    #    pprint(class_ids)\n",
    "    #    #pprint(label_partitions)\n",
    "    #    sys.exit()\n",
    "\n",
    "    # Delete Data\n",
    "    for i in range(len(class_ids)):\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            samples_per_class = int(np.floor(buffer_size / (task_id+1) / cpt))\n",
    "        else:\n",
    "            samples_per_class = int(np.ceil(buffer_size / (task_id+1) / cpt))\n",
    "\n",
    "        x = np.abs(samples_per_class - label_partitions[i].shape[0])\n",
    "        \n",
    "        if t == 7:\n",
    "            print('\\nClass:', i)\n",
    "            print('Difference:', x)\n",
    "            print('Current samples per class:', samples_per_class)\n",
    "            print('Previous samples per class:', label_partitions[i].shape[0])\n",
    "\n",
    "        data_partitions[i] = data_partitions[i][:-x]\n",
    "        label_partitions[i] = label_partitions[i][:-x]\n",
    "        \n",
    "    data_buffer = torch.cat(data_partitions)\n",
    "    labels_buffer = torch.cat(label_partitions)\n",
    "    \n",
    "    assert data_buffer.shape[0] == labels_buffer.shape[0]\n",
    "    return data_buffer, labels_buffer\n",
    "\n",
    "def add_data(data_buffer, labels_buffer, buffer_size, task_id, cpt):\n",
    "    \n",
    "    num_to_be_added = buffer_size - data_buffer.shape[0]\n",
    "    \n",
    "    for i in range(cpt*task_id, cpt*task_id + cpt):\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            samples_to_add = int(np.ceil(num_to_be_added / cpt))\n",
    "        else:\n",
    "            samples_to_add = int(np.floor(num_to_be_added / cpt))\n",
    "            \n",
    "        #if t == 6:\n",
    "        #    print('\\nClass:', i)\n",
    "        #    print('Difference:', x)\n",
    "        #    print('Current samples per class:', samples_per_class)\n",
    "        #    print('Previous samples per class:', label_partitions[i].shape[0])  \n",
    "\n",
    "        data_buffer = torch.cat((data_buffer, torch.randn(samples_to_add)))\n",
    "        labels_buffer = torch.cat((labels_buffer, i*torch.ones(samples_to_add)))\n",
    "    \n",
    "    assert data_buffer.shape[0] == labels_buffer.shape[0]\n",
    "    return data_buffer, labels_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e34e09f4-1cb6-4c05-bb46-cb1a02a553fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples in buffer: 2560\n",
      "Examples in buffer: 2560\n",
      "Examples in buffer: 2560\n",
      "Examples in buffer: 2560\n",
      "Examples in buffer: 2560\n"
     ]
    }
   ],
   "source": [
    "buffer_size = 2560\n",
    "n_tasks = 5\n",
    "cpt = 2\n",
    "\n",
    "for t in range(n_tasks):\n",
    "    \n",
    "    #print('\\nTask {}'.format(t))\n",
    "    \n",
    "    if t == 0:\n",
    "        labels_buffer = torch.tensor([i//(buffer_size/cpt) for i in range(buffer_size)]) # change denom when changing buffer_size and cpt\n",
    "        data_buffer = torch.randn(buffer_size)\n",
    "        print('Examples in buffer: {}'.format(torch.unique(labels_buffer, return_counts = True)[1].sum()))\n",
    "        unique, counts = torch.unique(labels_buffer, return_counts = True)\n",
    "        #print(unique)\n",
    "        #print(counts)\n",
    "        \n",
    "    else:\n",
    "        data_buffer, labels_buffer = delete_data(data_buffer, labels_buffer, buffer_size, t, cpt)\n",
    "        #if t == 7:\n",
    "        #    sys.exit()\n",
    "        data_buffer, labels_buffer = add_data(data_buffer, labels_buffer, buffer_size, t, cpt)\n",
    "        print('Examples in buffer: {}'.format(torch.unique(labels_buffer, return_counts = True)[1].sum()))\n",
    "        unique, counts = torch.unique(labels_buffer, return_counts = True)\n",
    "        #print(unique)\n",
    "        #print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f048e11-e8b9-4868-8942-099313b878b1",
   "metadata": {},
   "source": [
    "### Work to Generalize to N tasks with m cpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "897cafed-408b-4f05-8f3e-3919b277c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, labels, and buffer_size all exist relative to buffer object (i.e., self.examples, self.labels, self.buffer_size)\n",
    "# we could also make cpt relative to buffer as well since this typically doesn't change in literature\n",
    "\n",
    "def delete_data(data_buffer, labels_buffer, buffer_size, task_id, cpt):\n",
    "    \n",
    "    class_ids = torch.unique(labels_buffer)\n",
    "    data_partitions = [data_buffer[torch.argwhere(labels_buffer == class_id)].squeeze() for class_id in class_ids]\n",
    "    label_partitions = [labels_buffer[torch.argwhere(labels_buffer == class_id)].squeeze() for class_id in class_ids]\n",
    "\n",
    "    # Delete Data\n",
    "    #counter = 0\n",
    "    for i in range(len(class_ids)):\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            samples_per_class = int(np.floor(buffer_size / (task_id+1) / cpt))\n",
    "        else:\n",
    "            samples_per_class = int(np.ceil(buffer_size / (task_id+1) / cpt))\n",
    "        \n",
    "        # Edge Cases for when when memory buget runs out and each class only has one sample\n",
    "        if label_partitions[i].shape == torch.Size([]):\n",
    "            x = 0\n",
    "            label_partitions[i] = label_partitions[i].unsqueeze(0)\n",
    "        elif samples_per_class == 2:\n",
    "            x = 1\n",
    "        else:\n",
    "            # Normal situation when edge cases are not an issue\n",
    "            x = np.abs(samples_per_class - label_partitions[i].shape[0])\n",
    "            \n",
    "        # Another edge case scenario\n",
    "        if data_partitions[i].shape == torch.Size([]):\n",
    "            data_partitions[i] = data_partitions[i].unsqueeze(0)\n",
    "        \n",
    "        # x = 0 corresponds to nothing to delete when creates empty class arrays for storage\n",
    "        if x != 0:\n",
    "            data_partitions[i] = data_partitions[i][:-x]\n",
    "            label_partitions[i] = label_partitions[i][:-x]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    data_buffer = torch.cat(data_partitions)\n",
    "    labels_buffer = torch.cat(label_partitions)\n",
    "    \n",
    "    assert data_buffer.shape[0] == labels_buffer.shape[0]\n",
    "    return data_buffer, labels_buffer\n",
    "\n",
    "def add_data(data_buffer, labels_buffer, buffer_size, task_id, cpt):\n",
    "    \n",
    "    num_to_be_added = buffer_size - data_buffer.shape[0]\n",
    "    print('Added {} samples to buffer'.format(num_to_be_added))\n",
    "    \n",
    "    for i in range(cpt*task_id, cpt*task_id + cpt):\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            samples_to_add = int(np.floor(num_to_be_added / cpt))\n",
    "        else:\n",
    "            samples_to_add = int(np.ceil(num_to_be_added / cpt))\n",
    "\n",
    "        data_buffer = torch.cat((data_buffer, torch.randn(samples_to_add)))\n",
    "        labels_buffer = torch.cat((labels_buffer, i*torch.ones(samples_to_add)))\n",
    "    \n",
    "    assert data_buffer.shape[0] == labels_buffer.shape[0]\n",
    "    return data_buffer, labels_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "dbc71235-3930-4dff-951b-8b4bf39fcbc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task 0\n",
      "Examples in buffer: 5120\n",
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "tensor([512, 512, 512, 512, 512, 512, 512, 512, 512, 512])\n",
      "\n",
      "Task 1\n",
      "Added 2560 samples to buffer\n",
      "Examples in buffer: 5120\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15., 16., 17., 18., 19.])\n",
      "tensor([256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
      "        256, 256, 256, 256, 256, 256])\n",
      "\n",
      "Task 2\n",
      "Added 1710 samples to buffer\n",
      "Examples in buffer: 5120\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n",
      "        28., 29.])\n",
      "tensor([170, 171, 170, 171, 170, 171, 170, 171, 170, 171, 170, 171, 170, 171,\n",
      "        170, 171, 170, 171, 170, 171, 171, 171, 171, 171, 171, 171, 171, 171,\n",
      "        171, 171])\n",
      "\n",
      "Task 3\n",
      "Added 1280 samples to buffer\n",
      "Examples in buffer: 5120\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n",
      "        28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39.])\n",
      "tensor([128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
      "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
      "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128])\n",
      "\n",
      "Task 4\n",
      "Added 1020 samples to buffer\n",
      "Examples in buffer: 5120\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n",
      "        28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41.,\n",
      "        42., 43., 44., 45., 46., 47., 48., 49.])\n",
      "tensor([102, 103, 102, 103, 102, 103, 102, 103, 102, 103, 102, 103, 102, 103,\n",
      "        102, 103, 102, 103, 102, 103, 102, 103, 102, 103, 102, 103, 102, 103,\n",
      "        102, 103, 102, 103, 102, 103, 102, 103, 102, 103, 102, 103, 102, 102,\n",
      "        102, 102, 102, 102, 102, 102, 102, 102])\n",
      "\n",
      "Task 5\n",
      "Added 845 samples to buffer\n",
      "Examples in buffer: 5120\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n",
      "        28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41.,\n",
      "        42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55.,\n",
      "        56., 57., 58., 59.])\n",
      "tensor([85, 86, 85, 86, 85, 86, 85, 86, 85, 86, 85, 86, 85, 86, 85, 86, 85, 86,\n",
      "        85, 86, 85, 86, 85, 86, 85, 86, 85, 86, 85, 86, 85, 86, 85, 86, 85, 86,\n",
      "        85, 86, 85, 86, 85, 86, 85, 86, 85, 86, 85, 86, 85, 86, 84, 85, 84, 85,\n",
      "        84, 85, 84, 85, 84, 85])\n",
      "\n",
      "Task 6\n",
      "Added 710 samples to buffer\n",
      "Examples in buffer: 5120\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n",
      "        28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41.,\n",
      "        42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55.,\n",
      "        56., 57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69.])\n",
      "tensor([73, 74, 73, 74, 73, 74, 73, 74, 73, 74, 73, 74, 73, 74, 73, 74, 73, 74,\n",
      "        73, 74, 73, 74, 73, 74, 73, 74, 73, 74, 73, 74, 73, 74, 73, 74, 73, 74,\n",
      "        73, 74, 73, 74, 73, 74, 73, 74, 73, 74, 73, 74, 73, 74, 73, 74, 73, 74,\n",
      "        73, 74, 73, 74, 73, 74, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71])\n",
      "\n",
      "Task 7\n",
      "Added 640 samples to buffer\n",
      "Examples in buffer: 5120\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n",
      "        28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41.,\n",
      "        42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55.,\n",
      "        56., 57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69.,\n",
      "        70., 71., 72., 73., 74., 75., 76., 77., 78., 79.])\n",
      "tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64])\n",
      "\n",
      "Task 8\n",
      "Added 600 samples to buffer\n",
      "Examples in buffer: 5120\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n",
      "        28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41.,\n",
      "        42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55.,\n",
      "        56., 57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69.,\n",
      "        70., 71., 72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82., 83.,\n",
      "        84., 85., 86., 87., 88., 89.])\n",
      "tensor([56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57,\n",
      "        56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57,\n",
      "        56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57,\n",
      "        56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57, 56, 57,\n",
      "        56, 57, 56, 57, 56, 57, 56, 57, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60])\n",
      "\n",
      "Task 9\n",
      "Added 485 samples to buffer\n",
      "Examples in buffer: 5120\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n",
      "        28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41.,\n",
      "        42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55.,\n",
      "        56., 57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69.,\n",
      "        70., 71., 72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82., 83.,\n",
      "        84., 85., 86., 87., 88., 89., 90., 91., 92., 93., 94., 95., 96., 97.,\n",
      "        98., 99.])\n",
      "tensor([51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52,\n",
      "        51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52,\n",
      "        51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52,\n",
      "        51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52,\n",
      "        51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52, 51, 52,\n",
      "        48, 49, 48, 49, 48, 49, 48, 49, 48, 49])\n"
     ]
    }
   ],
   "source": [
    "buffer_size = 5120\n",
    "n_tasks = 10\n",
    "cpt = 10\n",
    "\n",
    "for t in range(n_tasks):\n",
    "    \n",
    "    print('\\nTask {}'.format(t))\n",
    "    \n",
    "    if t == 0:\n",
    "        labels_buffer = torch.tensor([i//(buffer_size/cpt) for i in range(buffer_size)]) # change denom when changing buffer_size and cpt\n",
    "        data_buffer = torch.randn(buffer_size)\n",
    "        print('Examples in buffer: {}'.format(torch.unique(labels_buffer, return_counts = True)[1].sum()))\n",
    "        unique, counts = torch.unique(labels_buffer, return_counts = True)\n",
    "        print(unique)\n",
    "        print(counts)\n",
    "        \n",
    "    else:\n",
    "        data_buffer, labels_buffer = delete_data(data_buffer, labels_buffer, buffer_size, t, cpt)\n",
    "        data_buffer, labels_buffer = add_data(data_buffer, labels_buffer, buffer_size, t, cpt)\n",
    "        print('Examples in buffer: {}'.format(torch.unique(labels_buffer, return_counts = True)[1].sum()))\n",
    "        unique, counts = torch.unique(labels_buffer, return_counts = True)\n",
    "        print(unique)\n",
    "        print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e91bca-c19d-4797-90c8-5317b2b74d40",
   "metadata": {},
   "source": [
    "#### If decide to run TinyImageNet Experiments with a buffer size of 100, we will have to further adapt to store more recently encountered examples as opposed to original examples. Although, an arguement can be that that we should keep the older samples since this are the samples most likely to be forgotten as compared to the most recent sampels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e1932-5f74-4c3e-96a5-bfbb531febcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
